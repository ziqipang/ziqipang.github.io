<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ziqi Pang</title>
  
  <meta name="author" content="Ziqi Pang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ziqi Pang (Â∫ûÂ≠êÂ•á)</name>
              </p>
              <p> I am a fourth-year CS Ph.D. student focusing on computer vision and machine learning at <a href="https://cs.illinois.edu/">University of Illinois Urabana-Champaign</a> (UIUC),
                  where my advisor is Prof. <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>. 
                  Before that, I graduated from <a href="https://english.pku.edu.cn/">Peking University</a> (PKU) with a Bachelor degree in Computer Science. 
              </p>
              <p>
                I interned at <a href="https://www.tri.global/our-work/machine-learning">Toyota Research Institute</a> (TRI) with Dr. <a href="https://pvtokmakov.github.io/home/">Pavel Tokmakov</a> during my Ph.D. study. 
                Prior to joining UIUC, I interned at <a href="https://www.ri.cmu.edu/">Carnegie Mellon University</a> (CMU) with Prof. <a href="http://www.cs.cmu.edu/~hebert/">Martial Hebert</a>,
                practiced research at <a href="https://english.pku.edu.cn/">Peking University</a> (PKU) with Prof. <a href="https://www.pkuvmc.com/">Shiliang Zhang</a>,
                and spent an exciting year at <a href="https://www.tusimple.com/">TuSimple</a> pushing the boundaries of autonomous driving guided by Dr. <a href="https://winsty.net/">Naiyan Wang</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:ziqip2@illinois.edu">Email</a> &nbsp/&nbsp
                <a href="./files/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=imNMDhoAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/ZiqiPang">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/ziqipang">Github</a> &nbsp/&nbsp
                <!-- <a href="https://www.zhihu.com/people/pang-zi-qi-40">Áü•‰πé</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ziqipang.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ziqipang.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research highlights <strong>perception in spatial (3D) and temporal (video) contexts</strong>, which is the cornerstone for embodied agents and digital assistants.
                Although being a so-called "perception" people, my ambition is <strong> how to unify generative modeling (LLMs, diffusion models, NeRF, etc.) with perception tasks </strong>.
                My goal is to unlock <strong>better scaling</strong>, <strong>better interactivity with humans</strong>,
                <strong>self-improvement and self-exploration</strong> of perception models from the generative capabilities of models.
              </p>
            </td>
          </tr>
        </tbody></table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  
  <tr onmouseout="ig2i_stop()" onmouseover="ig2i_start()"></tr>
    <td style="padding:20px;width:25%;vertical-align:top">
      <div class="two" id='ig2iimage'><img src="images/instruct_g2i.png" style="width:100%;height:auto"></div>
    </td>
    <td style="padding:30px;width:75%;vertical-align:middle">
      <papertitle>InstructG2I: Synthesizing Images from Multimodal Attributed Graphs</papertitle>
      <br>
      <a href="https://scholar.google.com/citations?user=FxvLU4sAAAAJ&hl=en">Bowen Jin</a>,
      <strong>Ziqi Pang</strong>,
      Bingjun Guo,
      <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>,
      <a href="https://cs.stanford.edu/people/jiaxuan/">Jiaxuan You</a>,
      <a href="https://hanj.cs.illinois.edu/">Jiawei Han</a>
      <br>
      <em> NeurIPS </em>, 2024
      <br>
      <a href="https://instructg2i.github.io/">Project Page</a>
      /
      <a href="https://github.com/PeterGriffinJin/InstructG2I">Code</a>
      /
      <a href="https://arxiv.org/abs/2410.07157">arXiv</a>
      <p>
      Text-to-image diffusion models can digest additional "graph" conditions about the relationships of entities, supporting more nuanced generation for recommendation systems, virtual arts, etc.
      </p>
    </td>
  </tr>
  
  <tr onmouseout="rmem_stop()" onmouseover="rmem_start()">
    <td style="padding:20px;width:25%;vertical-align:top">
      <div class="two" id='rmem_image'><img src="images/rmem.png" style="width:100%;height:auto"></div>
    </td>
    <td style="padding:30px;width:75%;vertical-align:middle">
      <papertitle>RMem: Restricted Memory Banks Improve Video Object Segmentation</papertitle>
      <br>
      <a href="https://scholar.google.com/citations?user=FxvLU4sAAAAJ&hl=en">Junbao Zhou</a>*,
      <strong>Ziqi Pang</strong>*,
      <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>
      <br>
      <em> CVPR </em>, 2024 (Winner at <em> ECCV 2024 </em> <a href="https://urldefense.com/v3/__https://www.votchallenge.net/vots2024/program.html__;!!DZ3fjg!7tRub8GFxgnvRhzrfLe9Y1xdb79tVDHm9HQa0UeY2_II6wIABp9tA3FM4io5GbViQm-8xxofi3Pyd9m1naaVcOb8Ww_AaSNE$">VOTS Challenge</a>)
      <br>
      <a href="https://restricted-memory.github.io/">Project Page</a>
      /
      <a href="https://github.com/Restricted-Memory/RMem">Code</a>
      /
      <a href="https://arxiv.org/abs/2406.08476">arXiv</a>
      <p>
      Simply bounding the size of memory banks improves VOS on challenging state-changes and long videos, indicating the importance of selecting relevant information from long contexts.
      </p>
    </td>
  </tr>

  <tr onmouseout="lm4vis_stop()" onmouseover="lm4vis_start()">
    <td style="padding:20px;width:25%;vertical-align:top">
      <div class="two" id='lidarsot_image'><img src="images/lm4vis.png" style="width:100%;height:auto"></div>
    </td>
    <td style="padding:30px;width:75%;vertical-align:middle">
      <papertitle>Frozen Transformers from Language Models are Effective Visual Encoder Layers</papertitle>
      <br>
      <strong>Ziqi Pang</strong>,
      <a href="https://ziyangxie.site/">Ziyang Xie</a>*,
      <a href="https://yunzeman.github.io/">Yunze Man</a>*,
      <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>
      <br>
      <em> ICLR </em>, 2024 (Spotlight) &nbsp
      <br>
      <a href="https://github.com/ziqipang/LM4VisualEncoding">Code</a>
      /
      <a href="https://arxiv.org/abs/2310.12973">arXiv</a>
      <p></p>
      <p>
      Frozen transformers from language models, though trained solely on textual data, can effectively improves diverse visual tasks by directly encoding visual tokens.
      </p>
    </td>
  </tr>

<tr onmouseout="mvmap_stop()" onmouseover="mvmap_start()">
  <td style="padding:20px;width:25%;vertical-align:top">
      <div class="two" id='mvmap_image'><video  width=120% height=120% muted autoplay loop>
      <source src="images/mvmap.mp4" type="video/mp4">
      Your browser does not support the video tag.
      </video></div>
  </td>
  <td style="padding:30px;width:75%;vertical-align:middle">
    <papertitle>MV-Map: Offboard HD-Map Generation with Multi-view Consistency</papertitle>
    <br>
    <a href="https://ziyangxie.site/">Ziyang Xie</a>*,
    <strong>Ziqi Pang</strong>*,
    <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>
    <br>
    <em>ICCV</em>, 2023 &nbsp
    <br>
    <a href="https://github.com/ZiYang-xie/MV-Map">Code</a>
    /
    <a href="https://arxiv.org/abs/2305.08851">arXiv</a>
    /
    <a href="https://www.youtube.com/watch?v=SN14oTyMFrk">Demo</a>
    <p></p>
    <p>
    MV-Map is the first offboard auto-labeling pipeline for HD-Maps, whose crust is to fuse BEV perception results guided by geometric cues from NeRFs. 
    </p>
  </td>
</tr>

<tr onmouseout="streaming_forecasting_stop()" onmouseover="streaming_forecasting()">
  <td style="padding:20px;width:25%;vertical-align:top">
      <div class="two" id='streaming_forecasting_image'><video  width=120% height=120% muted autoplay loop>
      <source src="images/streaming_forecasting.mp4" type="video/mp4">
      Your browser does not support the video tag.
      </video></div>
  </td>
  <td style="padding:30px;width:75%;vertical-align:middle">
    <papertitle>Streaming Motion Forecasting for Autonomous Driving</papertitle>
    <br>
    <strong>Ziqi Pang</strong>,
    <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>,
    <a href="https://mtli.github.io/">Mengtian Li</a>,
    <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>
    <br>
    <em>IROS</em>, 2023 &nbsp
    <br>
    <a href="https://github.com/ziqipang/StreamingForecasting">Code</a>
    /
    <a href="https://arxiv.org/abs/2310.01351"> arXiv </a>
    /
    Demo
    <p></p>
    <p>
    "Streaming forecasting" mitigates the gap between "snapshot-based" conventional motion forecasting and the streaming real-world traffic. 
    </p>
  </td>
</tr>

<tr onmouseout="pftrack_stop()" onmouseover="pftrack_start()">
  <td style="padding:20px;width:25%;vertical-align:top">
      <div class="two" id='pftrack_image'><video  width=120% height=120% muted autoplay loop>
      <source src="images/PF-Track/pf-track-demo.mp4" type="video/mp4">
      Your browser does not support the video tag.
      </video></div>
  </td>
  <td style="padding:30px;width:75%;vertical-align:middle">
    <papertitle>Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking (Alias: PF-Track)</papertitle>
    <br>
    <strong>Ziqi Pang</strong>,
    <a href="https://scholar.google.com/citations?user=_I3COxAAAAAJ&hl=en">Jie Li</a>,
    <a href="https://pvtokmakov.github.io/home/">Pavel Tokmakov</a>,
    <a href="https://scholar.google.com/citations?user=zdAyna8AAAAJ&hl=en&oi=ao">Dian Chen</a>,
    <a href="https://szagoruyko.github.io/">Sergey Zagoruyko</a>,
    <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>
    <br>
    <em>CVPR</em>, 2023 &nbsp
    <br>
    <a href="https://github.com/TRI-ML/PF-Track">Code</a>
    /
    <a href="https://arxiv.org/abs/2302.03802">arXiv</a>
    /
    <a href="https://youtu.be/eJghONb2AGg">Demo</a>
    <p></p>
    <p>
    PF-Track is an vision-centric 3D MOT framework that dramatically decreases ID-Switches with an end-to-end framework for autonomous driving. 
    </p>
  </td>
</tr>

<tr onmouseout="sst_stop()" onmouseover="sst_start()">
  <td style="padding:20px;width:25%;vertical-align:top">
    <div class="one">
      <div class="two" id='sst_image'><img src="images/sst.png" style="width:120%;height:120%"></div>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <papertitle>Embracing Single Stride 3D Object Detector with Sparse Transformer (Alias: SST)</papertitle>
    <br>
    <a href="https://lue.fan/">Lue Fan</a>,
    <strong>Ziqi Pang</strong>,
    <a href="https://tianyuanzhang.com/">Tianyuan Zhang</a>,
    <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>,
    <a href="https://hangzhaomit.github.io/">Hang Zhao</a>,
    <a href="http://happynear.wang/">Feng Wang</a>,
    <a href="https://winsty.net/">Naiyan Wang</a>,
    <a href="https://zhaoxiangzhang.net/">Zhaoxiang Zhang</a>
    <br>
    <em>CVPR</em>, 2022 &nbsp
    <br>
    <a href="https://github.com/TuSimple/SST">Code</a>
    /
    <a href="https://arxiv.org/abs/2112.06375">arXiv</a>
    <p></p>
    <p>
      SST emphasize the <em>small object sizes</em> and <em>sparsity</em> of point clouds. Its sparse transformers enlight new backbones for outdoor LiDAR-based detection. 
    </p>
  </td>
</tr>

<tr onmouseout="simpletrack_stop()" onmouseover="simpletrack_start()">
  <td style="padding:30px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='simpletrack_image'><img src="images/simpletrack_gif.gif" style="width:120%;height:auto" ></div>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <papertitle>SimpleTrack: Understanding and Rethinking 3D Multi-object Tracking </papertitle>
    <br>
    <strong>Ziqi Pang</strong>,
    <a href="https://scholar.google.com/citations?user=YJMbD38AAAAJ&hl">Zhichao Li</a>,
    <a href="https://winsty.net/">Naiyan Wang</a>
    <br>
    <em>ECCV Workshop</em>, 2022 &nbsp
    <br>
    <a href="https://github.com/TuSimple/SimpleTrack">Code</a>
    /
    <a href="https://arxiv.org/abs/2111.09621">arXiv</a> /
    <a href="https://patents.google.com/patent/US20230030496A1/en">Patent</a>
    <p></p>
    <p>
      SimpleTrack is simple-yet-effective 3D MOT system with more than 200 stars on GitHub. 
    </p>
  </td>
</tr>

<tr onmouseout="lidarsot_stop()" onmouseover="lidarsot_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='lidarsot_image'><img src="images/lidar_sot.png" style="width:130%;height:auto"></div>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <papertitle>Model-free Vehicle Tracking and State Estimation in Point Cloud Sequences (Alias: LiDAR-SOT)</papertitle>
    <br>
    <strong>Ziqi Pang</strong>,
    <a href="https://scholar.google.com/citations?user=YJMbD38AAAAJ&hl">Zhichao Li</a>,
    <a href="https://winsty.net/">Naiyan Wang</a>
    <br>
    <em>IROS</em>, 2021 &nbsp
    <br>
    <a href="https://github.com/TuSimple/LiDAR_SOT">Code</a>
    /
    <a href="https://arxiv.org/abs/2103.06028">arXiv</a> /
    <a href="https://www.youtube.com/watch?v=BpHixKs91i8">Demo</a>
    <p></p>
    <p>
    LiDAR-SOT is a LiDAR-based state estimation algorithm for both the onboard usage of redundancy system and offboard usage of auto-labeling.
    </p>
  </td>
</tr>

</tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;padding:40px"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Huge thanks to Jon Barron for proving the template for the page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
