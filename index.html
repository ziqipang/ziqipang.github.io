<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ziqi Pang</title>
  
  <meta name="author" content="Ziqi Pang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ziqi Pang (Â∫ûÂ≠êÂ•á)</name>
              </p>
              <p> I am a fourth-year CS Ph.D. student focusing on computer vision and machine learning at <a href="https://cs.illinois.edu/">University of Illinois Urabana-Champaign</a> (UIUC),
                  where my advisor is Prof. <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>. 
                  Before that, I graduated from <a href="https://english.pku.edu.cn/">Peking University</a> (PKU) with a Bachelor degree in Computer Science. 
              </p>
              <p>
                I interned at <a href="https://www.tri.global/our-work/machine-learning">Toyota Research Institute</a> (TRI) with Dr. <a href="https://pvtokmakov.github.io/home/">Pavel Tokmakov</a> during my Ph.D. study. 
                Prior to joining UIUC, I interned at <a href="https://www.ri.cmu.edu/">Carnegie Mellon University</a> (CMU) with Prof. <a href="http://www.cs.cmu.edu/~hebert/">Martial Hebert</a>,
                practiced research at <a href="https://english.pku.edu.cn/">Peking University</a> (PKU) with Prof. <a href="https://www.pkuvmc.com/">Shiliang Zhang</a>,
                and spent an exciting year at <a href="https://www.tusimple.com/">TuSimple</a> pushing the boundaries of autonomous driving guided by Dr. <a href="https://winsty.net/">Naiyan Wang</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:ziqip2@illinois.edu">Email</a> &nbsp/&nbsp
                <a href="./files/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=imNMDhoAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/ZiqiPang">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/ziqipang">Github</a> &nbsp/&nbsp
                <!-- <a href="https://www.zhihu.com/people/pang-zi-qi-40">Áü•‰πé</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ziqipang.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ziqipang.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                Enhance the Knowledge in <b>Generative Foundation Models</b> for <b>Embodied Perception in Long Videos</b>.
                I care about embodied perception in 2D, 3D and 4D. I envision generative pre-trained model as the critical component
                enabling their scaling and self-improvement.
              </p>
            </td>
          </tr>
        </tbody></table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  
  <tr onmouseout="randar_stop()" onmouseover="randar_start()"></tr>
  <td style="width:50%;vertical-align:middle;border: 2px solid #D9D9D9;">
    <div id='randar_image'><img src="images/randar_web_teaser.png" style="width:100%;height:auto"></div>
  </td>
    <td style="padding:30px;width:50%;vertical-align:middle">
      <papertitle>RandAR: Decoder-only Autoregressive Visual Generation in Random Orders</papertitle>
      <br>
      <strong>Ziqi Pang</strong>*,
      <a href="https://tianyuanzhang.com/">
        Tianyuan Zhang
      </a>*,
      <a href="https://luanfujun.com/">
        Fujun Luan
      </a>,
      <a href="https://yunzeman.github.io/">
        Yunze Man
      </a>,
      <a href="https://www.cs.unc.edu/~airsplay/">
        Hao Tan
      </a>,
      <a href="https://kai-46.github.io/website/">
        Kai Zhang
      </a>,
      <a href="https://billf.mit.edu/">
        William T. Freeman
      </a>,
      <a href="https://yxw.web.illinois.edu/">
        Yu-Xiong Wang
      </a>,
      <br>
      <em> In Submission to CVPR</em> 2025
      <br>
      <a href="https://rand-ar.github.io/">Project Page</a>
      /
      <a href="https://github.com/ziqipang/RandAR">Code</a>
      <!-- /
      <a href="https://arxiv.org/abs/2410.07157">arXiv</a> -->
      <p>
      We enable a GPT-style causal transformer to generate images in random orders, 
      which unlocks a series of new capabilities for decoder-only autoregressive models.
      </p>
    </td>
  </tr>
  
  <tr onmouseout="glus_stop()" onmouseover="glus_start()"></tr>
  <td style="width:50%;vertical-align:middle;border: 2px solid #D9D9D9;">
    <div id='glus_image'><img src="images/glus.png" style="width:100%;height:auto"></div>
  </td>
    <td style="padding:30px;width:50%;vertical-align:middle">
      <papertitle>GLUS: Global-Local Reasoning Unified into
        A Single Large Language Model for Video Segmentation</papertitle>
      <br>
      Lang Lin*,
      Xueyang Yu*,
      <strong>Ziqi Pang</strong>*,
      <a href="https://yxw.web.illinois.edu/">
        Yu-Xiong Wang
      </a>,
      <br>
      <em> In Submission to CVPR </em> 2025
      <br>
      <a href="https://glus-video.github.io/">Project Page</a>
      <!-- /
      <a href="https://arxiv.org/abs/2410.07157">arXiv</a> -->
      <p>
      We propose a simple yet effective MLLMs for language-instructed video segmentation. It emphasizes global-local video understanding and achieves SOTA performance on multiple benchmarks.
      </p>
    </td>
  </tr>

  <tr onmouseout="addp_stop()" onmouseover="addp_start()"></tr>
  <td style="width:50%;vertical-align:middle;border: 2px solid #D9D9D9;">
    <div id='addp_image'><img src="images/addp.png" style="width:100%;height:auto"></div>
  </td>
    <td style="padding:30px;width:50%;vertical-align:middle">
      <papertitle>Aligning Generative Denoising with Discriminative Objectives Unleashes Diffusion for Visual Perception</papertitle>
      <br>
      <strong>Ziqi Pang</strong>,
      <a href="https://scholar.google.com/citations?user=McTjAwwAAAAJ&hl=en">Xin Xu</a>,
      <a href="https://yxw.web.illinois.edu/">
        Yu-Xiong Wang
      </a>,
      <br>
      <em> In Submission to ICLR </em>, 2025
      <br>
      <p>
      Our paper answers several critical question on diffusion models for visual perception: 
      (1) how to train diffusion-based perception models, (2) how to utilize diffusion models as a unique interactive user interface.
      </p>
    </td>
  </tr>

  <tr onmouseout="ig2i_stop()" onmouseover="ig2i_start()"></tr>
    <td style="width:50%;vertical-align:middle;border: 2px solid #D9D9D9;">
      <div id='ig2iimage'><img src="images/2024_instructg2i_neurips2024_ziqi.png" style="width:100%;height:auto"></div>
    </td>
    <td style="padding:20px;width:50%;vertical-align:top">
      <papertitle>InstructG2I: Synthesizing Images from Multimodal Attributed Graphs</papertitle>
      <br>
      <a href="https://scholar.google.com/citations?user=FxvLU4sAAAAJ&hl=en">Bowen Jin</a>,
      <strong>Ziqi Pang</strong>,
      Bingjun Guo,
      <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>,
      <a href="https://cs.stanford.edu/people/jiaxuan/">Jiaxuan You</a>,
      <a href="https://hanj.cs.illinois.edu/">Jiawei Han</a>
      <br>
      <em> NeurIPS </em>, 2024
      <br>
      <a href="https://instructg2i.github.io/">Project Page</a>
      /
      <a href="https://github.com/PeterGriffinJin/InstructG2I">Code</a>
      /
      <a href="https://arxiv.org/abs/2410.07157">arXiv</a>
      <p>
      Using the relationships between entities, we can better control the generation of images with multi-modal graphs.
      </p>
    </td>
  </tr>
  
  <tr onmouseout="rmem_stop()" onmouseover="rmem_start()">
    <td style="width:50%;vertical-align:middle;border: 2px solid #D9D9D9;">
      <div id='rmem_image'><img src="images/2024_rmem_cvpr2024_ziqipang.png" style="width:100%;height:auto"></div>
    </td>
    <td style="padding:20px;width:50%;vertical-align:top">
      <papertitle>RMem: Restricted Memory Banks Improve Video Object Segmentation</papertitle>
      <br>
      <a href="https://scholar.google.com/citations?user=FxvLU4sAAAAJ&hl=en">Junbao Zhou</a>*,
      <strong>Ziqi Pang</strong>*,
      <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>
      <br>
      <em> CVPR </em>, 2024 (Winner at <em> ECCV 2024 </em> <a href="https://urldefense.com/v3/__https://www.votchallenge.net/vots2024/program.html__;!!DZ3fjg!7tRub8GFxgnvRhzrfLe9Y1xdb79tVDHm9HQa0UeY2_II6wIABp9tA3FM4io5GbViQm-8xxofi3Pyd9m1naaVcOb8Ww_AaSNE$">VOTS Challenge</a>)
      <br>
      <a href="https://restricted-memory.github.io/">Project Page</a>
      /
      <a href="https://github.com/Restricted-Memory/RMem">Code</a>
      /
      <a href="https://arxiv.org/abs/2406.08476">arXiv</a>
      <p>
      Managing memory banks better significantly improves VOS on challenging state-changes and long videos. Similar strategy is also adopted in SAM2 later.
      </p>
    </td>
  </tr>

  <tr onmouseout="lm4vis_stop()" onmouseover="lm4vis_start()">
    <td style="width:50%;vertical-align:middle;border: 2px solid #D9D9D9;">
      <div id='lm4vis_image'><img src="images/2024_LM4VE_iclr2024_ziqi.png" style="width:100%;height:auto"></div>
    </td>
    <td style="padding:20px;width:50%;vertical-align:top">
      <papertitle>Frozen Transformers from Language Models are Effective Visual Encoder Layers</papertitle>
      <br>
      <strong>Ziqi Pang</strong>,
      <a href="https://ziyangxie.site/">Ziyang Xie</a>*,
      <a href="https://yunzeman.github.io/">Yunze Man</a>*,
      <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>
      <br>
      <em> ICLR </em>, 2024 (Spotlight) &nbsp
      <br>
      <a href="https://github.com/ziqipang/LM4VisualEncoding">Code</a>
      /
      <a href="https://arxiv.org/abs/2310.12973">arXiv</a>
      <p></p>
      <p>
      Frozen transformers from language models, though trained solely on textual data, can effectively improves diverse visual tasks by directly encoding visual tokens.
      It is an essential step for my research on "generative models benefiting perception."
      </p>
    </td>
  </tr>

<tr onmouseout="mvmap_stop()" onmouseover="mvmap_start()">
  <td style="width:50%;vertical-align:middle;border: 2px solid #D9D9D9;">
    <div id='mvmap_image'><img src="images/2023_MVMap_iccv2023.gif" style="width:100%;height:auto"></div>
  </td>
  <td style="padding:20px;width:50%;vertical-align:top">
    <papertitle>MV-Map: Offboard HD-Map Generation with Multi-view Consistency</papertitle>
    <br>
    <a href="https://ziyangxie.site/">Ziyang Xie</a>*,
    <strong>Ziqi Pang</strong>*,
    <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>
    <br>
    <em>ICCV</em>, 2023 &nbsp
    <br>
    <a href="https://github.com/ZiYang-xie/MV-Map">Code</a>
    /
    <a href="https://arxiv.org/abs/2305.08851">arXiv</a>
    /
    <a href="https://www.youtube.com/watch?v=SN14oTyMFrk">Demo</a>
    <p></p>
    <p>
    MV-Map is the first offboard auto-labeling pipeline for HD-Maps, whose crust is to fuse BEV perception results guided by geometric cues from NeRFs. 
    </p>
  </td>
</tr>

<tr onmouseout="streaming_forecasting_stop()" onmouseover="streaming_forecasting()">
  <td style="width:50%;vertical-align:middle;border: 2px solid #D9D9D9;">
      <div id='streaming_forecasting_image'><img src="images/2023_streaming_forecasting_iros2023_ziqi.gif" style="width:100%;height:auto"></div>
  </td>
  <td style="padding:20px;width:50%;vertical-align:top">
    <papertitle>Streaming Motion Forecasting for Autonomous Driving</papertitle>
    <br>
    <strong>Ziqi Pang</strong>,
    <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>,
    <a href="https://mtli.github.io/">Mengtian Li</a>,
    <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>
    <br>
    <em>IROS</em>, 2023 &nbsp
    <br>
    <a href="https://github.com/ziqipang/StreamingForecasting">Code</a>
    /
    <a href="https://arxiv.org/abs/2310.01351"> arXiv </a>
    /
    Demo
    <p></p>
    <p>
    "Streaming forecasting" mitigates the gap between "snapshot-based" conventional motion forecasting and the streaming real-world traffic. 
    </p>
  </td>
</tr>

<tr onmouseout="pftrack_stop()" onmouseover="pftrack_start()">
  <td style="width:50%;vertical-align:middle;border: 2px solid #D9D9D9;">
    <div id='pftrack_image'><img src="images/2023_PF-Track_cvpr2023_ziqi.gif" style="width:100%;height:auto"></div>
  </td>
  <td style="padding:20px;width:50%;vertical-align:top">
    <papertitle>Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking (Alias: PF-Track)</papertitle>
    <br>
    <strong>Ziqi Pang</strong>,
    <a href="https://scholar.google.com/citations?user=_I3COxAAAAAJ&hl=en">Jie Li</a>,
    <a href="https://pvtokmakov.github.io/home/">Pavel Tokmakov</a>,
    <a href="https://scholar.google.com/citations?user=zdAyna8AAAAJ&hl=en&oi=ao">Dian Chen</a>,
    <a href="https://szagoruyko.github.io/">Sergey Zagoruyko</a>,
    <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>
    <br>
    <em>CVPR</em>, 2023 &nbsp
    <br>
    <a href="https://github.com/TRI-ML/PF-Track">Code</a>
    /
    <a href="https://arxiv.org/abs/2302.03802">arXiv</a>
    /
    <a href="https://youtu.be/eJghONb2AGg">Demo</a>
    <p></p>
    <p>
    PF-Track is an vision-centric 3D MOT framework that dramatically decreases ID-Switches by 90% with an end-to-end framework for autonomous driving. 
    </p>
  </td>
</tr>

<tr onmouseout="sst_stop()" onmouseover="sst_start()">
  <td style="width:50%;vertical-align:middle;border: 2px solid #D9D9D9;">
    <div id='sst_image'><img src="images/sst_web_teaser.png" style="width:100%;height:auto"></div>
    </div>
  </td>
  <td style="padding:20px;width:50%;vertical-align:middle">
    <papertitle>Embracing Single Stride 3D Object Detector with Sparse Transformer (Alias: SST)</papertitle>
    <br>
    <a href="https://lue.fan/">Lue Fan</a>,
    <strong>Ziqi Pang</strong>,
    <a href="https://tianyuanzhang.com/">Tianyuan Zhang</a>,
    <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>,
    <a href="https://hangzhaomit.github.io/">Hang Zhao</a>,
    <a href="http://happynear.wang/">Feng Wang</a>,
    <a href="https://winsty.net/">Naiyan Wang</a>,
    <a href="https://zhaoxiangzhang.net/">Zhaoxiang Zhang</a>
    <br>
    <em>CVPR</em>, 2022 &nbsp
    <br>
    <a href="https://github.com/TuSimple/SST">Code</a>
    /
    <a href="https://arxiv.org/abs/2112.06375">arXiv</a>
    <p></p>
    <p>
      SST emphasize the <em>small object sizes</em> and <em>sparsity</em> of point clouds. Its sparse transformers enlight new backbones for outdoor LiDAR-based detection. 
    </p>
  </td>
</tr>

<tr onmouseout="simpletrack_stop()" onmouseover="simpletrack_start()">
  <td style="width:50%;vertical-align:middle;border: 2px solid #D9D9D9;">
    <div id='simpletrack_image'><img src="images/2022_SST_cvpr2022_ziqi.gif" style="width:100%;height:auto"></div>
  </td>
  <td style="padding:20px;width:50%;vertical-align:top">
    <papertitle>SimpleTrack: Understanding and Rethinking 3D Multi-object Tracking </papertitle>
    <br>
    <strong>Ziqi Pang</strong>,
    <a href="https://scholar.google.com/citations?user=YJMbD38AAAAJ&hl">Zhichao Li</a>,
    <a href="https://winsty.net/">Naiyan Wang</a>
    <br>
    <em>ECCV Workshop</em>, 2022 &nbsp
    <br>
    <a href="https://github.com/TuSimple/SimpleTrack">Code</a>
    /
    <a href="https://arxiv.org/abs/2111.09621">arXiv</a> /
    <a href="https://patents.google.com/patent/US20230030496A1/en">Patent</a>
    <p></p>
    <p>
      SimpleTrack is simple-yet-effective 3D MOT system. It is one of the most widely adopted 3D MOT baseline worldwide.
    </p>
  </td>
</tr>

<tr onmouseout="lidarsot_stop()" onmouseover="lidarsot_start()">
  <td style="width:50%;vertical-align:middle;border: 2px solid #D9D9D9;">
    <div id='lidarsot_image'><img src="images/lidar_sot_web_teaser.png" style="width:100%;height:auto"></div>
  </td>
  <td style="padding:20px;width:50%;vertical-align:middle">
    <papertitle>Model-free Vehicle Tracking and State Estimation in Point Cloud Sequences (Alias: LiDAR-SOT)</papertitle>
    <br>
    <strong>Ziqi Pang</strong>,
    <a href="https://scholar.google.com/citations?user=YJMbD38AAAAJ&hl">Zhichao Li</a>,
    <a href="https://winsty.net/">Naiyan Wang</a>
    <br>
    <em>IROS</em>, 2021 &nbsp
    <br>
    <a href="https://github.com/TuSimple/LiDAR_SOT">Code</a>
    /
    <a href="https://arxiv.org/abs/2103.06028">arXiv</a> /
    <a href="https://www.youtube.com/watch?v=BpHixKs91i8">Demo</a>
    <p></p>
    <p>
    LiDAR-SOT is a LiDAR-based data flywheeel and auto-labeling pipeline for autonomous driving.
    </p>
  </td>
</tr>

</tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;padding:40px"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Huge thanks to Jon Barron for proving the template for the page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
