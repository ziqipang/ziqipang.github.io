<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ziqi Pang</title>
  
  <meta name="author" content="Ziqi Pang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ziqi Pang (Â∫ûÂ≠êÂ•á)</name>
              </p>
              <p> I am a second-year CS Ph.D. student focusing on computer vision and machine learning at <a href="https://cs.illinois.edu/">University of Illinois Urabana-Champaign</a> (UIUC),
                  where my advisor is Prof. <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>. 
                  Before that, I graduated from <a href="https://english.pku.edu.cn/">Peking University</a> (PKU) with a Bachelor degree in Computer Science. 
              </p>
              <p>
                I interned at <a href="https://www.tri.global/our-work/machine-learning">Toyota Research Institute</a> (TRI) with Dr. <a href="https://pvtokmakov.github.io/home/">Pavel Tokmakov</a> during my Ph.D. study. 
                Prior to joining UIUC, I interned at <a href="https://www.ri.cmu.edu/">Carnegie Mellon University</a> (CMU) with Prof. <a href="http://www.cs.cmu.edu/~hebert/">Martial Hebert</a>,
                practiced research at <a href="https://english.pku.edu.cn/">Peking University</a> (PKU) with Prof. <a href="https://www.pkuvmc.com/">Shiliang Zhang</a>,
                and spent an exciting year at <a href="https://www.tusimple.com/">TuSimple</a> pushing the boundaries of autonomous driving guided by Dr. <a href="https://winsty.net/">Naiyan Wang</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:ziqip2@illinois.edu">Email</a> &nbsp/&nbsp
                <a href="./files/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=imNMDhoAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/ZiqiPang">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/ziqipang">Github</a> &nbsp/&nbsp
                <!-- <a href="https://www.zhihu.com/people/pang-zi-qi-40">Áü•‰πé</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ziqipang.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ziqipang.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research concentrates on <strong>3D perception</strong> and <strong>prediction</strong> in computer vision. <strong>Autonomous driving</strong> is my major handle because of its multi-facet challenges and huge impacts to the real-world. For instance, my expertise covers:
                <ul>
                  <li>Diverse components (detection, tracking, and prediction)</li>
                  <li>Multiple sensors (cameras and LiDAR)</li>
                  <li>Single/multi-frame reasoning (detection and tracking)</li>
                  <li>Onboard/offboard applications (BEV perception and auto-labeling)</li>
                </ul>
                Meanwhile, I am also exploring these techniques for other applicational scenarios and real-world products. Please emails me if you wish to collaborate for research, start-up, consulting, etc. 
              </p>
            </td>
          </tr>
        </tbody></table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

  <tr onmouseout="lm4vis_stop()" onmouseover="lm4vis_start()">
    <td style="padding:20px;width:25%;vertical-align:top">
      <div class="two" id='lidarsot_image'><img src="images/lm4vis.png" style="width:100%;height:auto"></div>
    </td>
    <td style="padding:30px;width:75%;vertical-align:middle">
      <papertitle>Frozen Transformers from Language Models are Effective Visual Encoder Layers</papertitle>
      <br>
      <strong>Ziqi Pang</strong>,
      <a href="https://ziyangxie.site/">Ziyang Xie</a>*,
      <a href="https://yunzeman.github.io/">Yunze Man</a>*,
      <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>
      <br>
      <em> Preprint </em>, 2023 &nbsp
      <br>
      <a href="https://github.com/ziqipang/LM4VisualEncoding">Code</a>
      /
      <a href="https://arxiv.org/abs/2310.12973">arXiv</a>
      <p></p>
      <p>
      Frozen transformers from language models, though trained solely on textual data, can effectively improves diverse visual tasks by directly encoding visual tokens.
      </p>
    </td>
  </tr>

<tr onmouseout="mvmap_stop()" onmouseover="mvmap_start()">
  <td style="padding:20px;width:25%;vertical-align:top">
      <div class="two" id='mvmap_image'><video  width=120% height=120% muted autoplay loop>
      <source src="images/mvmap.mp4" type="video/mp4">
      Your browser does not support the video tag.
      </video></div>
  </td>
  <td style="padding:30px;width:75%;vertical-align:middle">
    <papertitle>MV-Map: Offboard HD-Map Generation with Multi-view Consistency (Alias: MV-Map)</papertitle>
    <br>
    <a href="https://ziyangxie.site/">Ziyang Xie</a>*,
    <strong>Ziqi Pang</strong>*,
    <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>
    <br>
    <em>ICCV</em>, 2023 &nbsp
    <br>
    <a href="https://github.com/ZiYang-xie/MV-Map">Code</a>
    /
    <a href="https://arxiv.org/abs/2305.08851">arXiv</a>
    /
    <a href="https://www.youtube.com/watch?v=SN14oTyMFrk">Demo</a>
    <p></p>
    <p>
    MV-Map is the first offboard auto-labeling pipeline for HD-Maps, whose crust is to fuse BEV perception results guided by geometric cues from NeRFs. 
    </p>
  </td>
</tr>

<tr onmouseout="streaming_forecasting_stop()" onmouseover="streaming_forecasting()">
  <td style="padding:20px;width:25%;vertical-align:top">
      <div class="two" id='streaming_forecasting_image'><video  width=120% height=120% muted autoplay loop>
      <source src="images/streaming_forecasting.mp4" type="video/mp4">
      Your browser does not support the video tag.
      </video></div>
  </td>
  <td style="padding:30px;width:75%;vertical-align:middle">
    <papertitle>Streaming Motion Forecasting for Autonomous Driving</papertitle>
    <br>
    <strong>Ziqi Pang</strong>,
    <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>,
    <a href="https://mtli.github.io/">Mengtian Li</a>,
    <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>
    <br>
    <em>IROS</em>, 2023 &nbsp
    <br>
    <a href="https://github.com/ziqipang/StreamingForecasting">Code</a>
    /
    <a href="https://arxiv.org/abs/2310.01351"> arXiv </a>
    /
    Demo
    <p></p>
    <p>
    "Streaming forecasting" mitigates the gap between "snapshot-based" conventional motion forecasting and the streaming real-world traffic. 
    </p>
  </td>
</tr>

<tr onmouseout="pftrack_stop()" onmouseover="pftrack_start()">
  <td style="padding:20px;width:25%;vertical-align:top">
      <div class="two" id='pftrack_image'><video  width=120% height=120% muted autoplay loop>
      <source src="images/PF-Track/pf-track-demo.mp4" type="video/mp4">
      Your browser does not support the video tag.
      </video></div>
  </td>
  <td style="padding:30px;width:75%;vertical-align:middle">
    <papertitle>Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera 3D Multi-Object Tracking (Alias: PF-Track)</papertitle>
    <br>
    <strong>Ziqi Pang</strong>,
    <a href="https://scholar.google.com/citations?user=_I3COxAAAAAJ&hl=en">Jie Li</a>,
    <a href="https://pvtokmakov.github.io/home/">Pavel Tokmakov</a>,
    <a href="https://scholar.google.com/citations?user=zdAyna8AAAAJ&hl=en&oi=ao">Dian Chen</a>,
    <a href="https://szagoruyko.github.io/">Sergey Zagoruyko</a>,
    <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>
    <br>
    <em>CVPR</em>, 2023 &nbsp
    <br>
    <a href="https://github.com/TRI-ML/PF-Track">Code</a>
    /
    <a href="https://arxiv.org/abs/2302.03802">arXiv</a>
    /
    <a href="https://youtu.be/eJghONb2AGg">Demo</a>
    <p></p>
    <p>
    PF-Track is an vision-centric 3D MOT framework that dramatically decreases ID-Switches with an end-to-end framework for autonomous driving. 
    </p>
  </td>
</tr>

<tr onmouseout="sst_stop()" onmouseover="sst_start()">
  <td style="padding:20px;width:25%;vertical-align:top">
    <div class="one">
      <div class="two" id='sst_image'><img src="images/sst.png" style="width:120%;height:120%"></div>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <papertitle>Embracing Single Stride 3D Object Detector with Sparse Transformer (Alias: SST)</papertitle>
    <br>
    <a href="https://lue.fan/">Lue Fan</a>,
    <strong>Ziqi Pang</strong>,
    <a href="https://tianyuanzhang.com/">Tianyuan Zhang</a>,
    <a href="https://yxw.web.illinois.edu/">Yu-Xiong Wang</a>,
    <a href="https://hangzhaomit.github.io/">Hang Zhao</a>,
    <a href="http://happynear.wang/">Feng Wang</a>,
    <a href="https://winsty.net/">Naiyan Wang</a>,
    <a href="https://zhaoxiangzhang.net/">Zhaoxiang Zhang</a>
    <br>
    <em>CVPR</em>, 2022 &nbsp
    <br>
    <a href="https://github.com/TuSimple/SST">Code</a>
    /
    <a href="https://arxiv.org/abs/2112.06375">arXiv</a>
    <p></p>
    <p>
      SST emphasize the <em>small object sizes</em> and <em>sparsity</em> of point clouds. Its sparse transformers enlight new backbones for outdoor LiDAR-based detection. 
    </p>
  </td>
</tr>

<tr onmouseout="simpletrack_stop()" onmouseover="simpletrack_start()">
  <td style="padding:30px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='simpletrack_image'><img src="images/simpletrack_gif.gif" style="width:120%;height:auto" ></div>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <papertitle>SimpleTrack: Understanding and Rethinking 3D Multi-object Tracking (Alias: SimpleTrack)</papertitle>
    <br>
    <strong>Ziqi Pang</strong>,
    <a href="https://scholar.google.com/citations?user=YJMbD38AAAAJ&hl">Zhichao Li</a>,
    <a href="https://winsty.net/">Naiyan Wang</a>
    <br>
    <em>ECCV Workshop</em>, 2022 &nbsp
    <br>
    <a href="https://github.com/TuSimple/SimpleTrack">Code</a>
    /
    <a href="https://arxiv.org/abs/2111.09621">arXiv</a> /
    <a href="https://patents.google.com/patent/US20230030496A1/en">Patent</a>
    <p></p>
    <p>
      SimpleTrack is simple-yet-effective 3D MOT system with more than 200 stars on GitHub. 
    </p>
  </td>
</tr>

<tr onmouseout="lidarsot_stop()" onmouseover="lidarsot_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='lidarsot_image'><img src="images/lidar_sot.png" style="width:130%;height:auto"></div>
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <papertitle>Model-free Vehicle Tracking and State Estimation in Point Cloud Sequences (Alias: LiDAR-SOT)</papertitle>
    <br>
    <strong>Ziqi Pang</strong>,
    <a href="https://scholar.google.com/citations?user=YJMbD38AAAAJ&hl">Zhichao Li</a>,
    <a href="https://winsty.net/">Naiyan Wang</a>
    <br>
    <em>IROS</em>, 2021 &nbsp
    <br>
    <a href="https://github.com/TuSimple/LiDAR_SOT">Code</a>
    /
    <a href="https://arxiv.org/abs/2103.06028">arXiv</a> /
    <a href="https://www.youtube.com/watch?v=BpHixKs91i8">Demo</a>
    <p></p>
    <p>
    LiDAR-SOT is a LiDAR-based state estimation algorithm for both the onboard usage of redundancy system and offboard usage of auto-labeling.
    </p>
  </td>
</tr>

</tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;padding:40px"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Huge thanks to Jon Barron for proving the template for the page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
